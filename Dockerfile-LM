FROM ghcr.io/ggml-org/llama.cpp:server

# Copy pre-downloaded model into the image
COPY models/ /models/

# Default command with the baked-in model
CMD ["--model", "/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf", \
     "--host", "0.0.0.0", \
     "--port", "8080", \
     "--ctx-size", "4096", \
     "--n-gpu-layers", "0"]

EXPOSE 8080
